<!DOCTYPE html>
<html lang="en">

<script>
  window.MathJax = {
    tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Research</title>
</head>

<body>
  <h2>Research</h2>
  <br>

  <h5 style='width: 70%'>On the class of exponential statistical structures of type B</h5>
    <p><b>Oleksandr Volkov</b>, Yurii Volkov<br>
      <i>Bulletin of Taras Shevchenko National University of Kyiv</i>, 2025 <br>
      <a href='https://arxiv.org/abs/2501.02703' id='link'>[link]</a> 
    </p>
  
    <button class='toggle-button' onclick="showAbstract('abstract4')"><i class="fa-solid fa-caret-down"></i>
      Abstract</button>
  
    <p class='abstract' id="abstract4">The article is devoted to the study of exponential statistical structures of type B, which constitute an important subclass of
      exponential families of probability distributions. This class is characterized by a number of analytical and probabilistic properties
      that make it a convenient tool for solving both theoretical and applied problems in mathematical statistics. The relevance of this
      research lies in the need to generalize known classes of distributions and to develop a unified framework for their analysis, which
      is essential for applications in stochastic modeling, machine learning, and financial mathematics.
      The paper proposes a formal definition of type B distributions based on the Laplace transform of dominating measures
      and a system of functional-differential equations describing their structure. Necessary and sufficient conditions for a statistical
      structure to belong to class B are established, and it is proved that such structures can be represented through a dominating
      measure with an explicit Laplace transform. The obtained results make it possible to describe a wide range of well-known
      one-dimensional and multivariate distributions, including the binomial, Poisson, normal, gamma, polynomial, and logarithmic
      distributions, as well as specific cases such as the Borel–Tanner distribution and random walk distributions.
      Particular attention is given to the proof of structural theorems that determine the stability of class B under linear transformations and the addition of independent random vectors. It is shown that if a distribution belongs to class B, its linear
      transformations and sums also belong to this class with the corresponding parameters. Recursive relations for initial and central moments as well as for semi-invariants are obtained, providing an efficient analytical and computational framework for their
      evaluation.
      Furthermore, the “tails” of type B distributions are investigated using the properties of the Laplace transform. As a result,
      new exponential inequalities for estimating the probabilities of large deviations are derived, which extend classical approaches
      to the analysis of statistical distributions. The obtained results can be applied in theoretical studies and in practical problems of
      stochastic modeling.</p>
  
    <br>

  <h5 style='width: 70%'>On a power series distribution with mean
    parameterization</h5>
  <p><b>Oleksandr Volkov</b> and Nataliia Voinalovych <br>
    <i>Scientific Bulletin of Uzhhorod University. Series of Mathematics and Informatics</i>, 2025 <br>
    <a href='https://arxiv.org/abs/2409.12928' id='link'>[link]</a>
  </p>

  <button class='toggle-button' onclick="showAbstract('abstract3')"><i class="fa-solid fa-caret-down"></i>
    Abstract</button>

  <p class="abstract" id="abstract3">
The article examines the distribution of the power series of the function $w(y) = (1 + \sqrt{1 - y})^{-1/2}$. The distribution of the considered function into a power series is obtained: $(1 + \sqrt{1-y})^{-1/2} = \sum_{m=0}^{\infty} \frac{(4m)!}{(2m)!(2m+1)! \, 2^{4m}} y^m$. The dispersion function is found $v(x) = x(2x+1)(4x+1)$, $x > 0$. A distribution with mean parameterization is constructed: $\Pr(\xi = k) = \frac{(4k+1)}{2^k}\binom{2k}{k}x^k (2k+1)^{k+\frac{1}{2}} (4k+1)^{-2k-\frac{3}{2}}, \quad x > 0$. It is proved that the raw moments $\alpha_m$, central moments $\mu_m$, cumulants $\chi_m$, $m = 1,2,\dots$, satisfy the recurrence relations: $\alpha_{m+1} = x\alpha_m + v(x) \frac{d\alpha_m}{dx}$, $\alpha_0 = 1$, $\alpha_1 = x$; $\mu_{m+1} = m\mu_m + v(x)\frac{d\mu_m}{dx}$, $\mu_0 = 1$, $\mu_1 = 0$; $\chi_{m+1} = v(x)\frac{d\chi_m}{dx}$, $\chi_1 = x$.
</p>

  <br>

  <h5>Boosting e-BH via conditional calibration</h5>
  <p><b>Junu Lee</b> and Zhimei Ren <br>
    <a href='https://arxiv.org/abs/2404.17562' id='link'>[link]</a>
    <a href='https://github.com/leejunu/e-bh-cc' id='link'>[code]</a>
  </p>

  <button class='toggle-button' onclick="showAbstract('abstract1')"><i class="fa-solid fa-caret-down"></i>
    Abstract</button>

  <p class='abstract' id="abstract1">The e-BH procedure is an e-value-based multiple testing procedure that provably
    controls the false discovery rate (FDR) under any dependence structure between the e-values. Despite this appealing
    theoretical FDR control guarantee, the e-BH procedure often suffers from low power in practice. In this paper, we
    propose a general framework that boosts the power of e-BH without sacrificing its FDR control under arbitrary
    dependence. This is achieved by the technique of conditional calibration, where we take as input the e-values and
    calibrate them to be a set of "boosted e-values" that are guaranteed to be no less -- and are often more -- powerful
    than the original ones. Our general framework is explicitly instantiated in three classes of multiple testing
    problems: (1) testing under parametric models, (2) conditional independence testing under the model-X setting, and
    (3) model-free conformalized selection. Extensive numerical experiments show that our proposed method significantly
    improves the power of e-BH while continuing to control the FDR. We also demonstrate the effectiveness of our method
    through an application to an observational study dataset for identifying individuals whose counterfactuals satisfy
    certain properties.</p>

  <br>

  <h5 style='width: 70%'>Navigating Data Heterogeneity in Federated Learning: A Semi-Supervised Federated Object Detection
  </h5>
  <p>Taehyeon Kim, Eric Lin, <b>Junu Lee</b>, Christian Lau, and Vaikkunth Mugunthan <br>
    <i>Neural Information Processing Systems (NeurIPS)</i>, 2023 <br>
    <a href='https://proceedings.neurips.cc/paper_files/paper/2023/hash/066e4dbfeccb5dc2851acd5eca584937-Abstract-Conference.html' id='link'>[link]</a>
  </p>

  <button class='toggle-button' onclick="showAbstract('abstract2')"><i class="fa-solid fa-caret-down"></i>
    Abstract</button>

  <p class='abstract' id="abstract2">Federated Learning (FL) has emerged as a potent framework for training models
    across distributed data sources while maintaining data privacy. Nevertheless, it faces challenges with limited
    high-quality labels and non-IID client data, particularly in applications like autonomous driving. To address these
    hurdles, we navigate the uncharted waters of Semi-Supervised Federated Object Detection (SSFOD). We present a
    pioneering SSFOD framework, designed for scenarios where labeled data reside only at the server while clients
    possess unlabeled data. Notably, our method represents the inaugural implementation of SSFOD for clients with 0%
    labeled non-IID data, a stark contrast to previous studies that maintain some subset of labels at each client. We
    propose FedSTO, a two-stage strategy encompassing Selective Training followed by Orthogonally enhanced
    full-parameter training, to effectively address data shift (e.g. weather conditions) between server and clients. Our
    contributions include selectively refining the backbone of the detector to avert overfitting, orthogonality
    regularization to boost representation divergence, and local EMA-driven pseudo label assignment to yield
    high-quality pseudo labels. Extensive validation on prominent autonomous driving datasets (BDD100K, Cityscapes, and
    SODA10M) attests to the efficacy of our approach, demonstrating state-of-the-art results. Remarkably, FedSTO, using
    just 20-30% of labels, performs nearly as well as fully-supervised centralized training methods.</p>


  <br>
</body>
</html>
